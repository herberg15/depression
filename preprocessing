import json
from pathlib import Path


POS_IN = Path()
NEG_IN = Path()

def count_stats(directory_path: Path):
    
    if not directory_path.is_dir():
        print(f"HATA: '{directory_path}' dizini bulunamadı!")
        return 0, 0
        
  
    user_files = list(directory_path.glob('*.json'))
    user_count = len(user_files)
    tweet_count = 0
    
    print(f"'{directory_path.name}' dizininde {user_count} kullanıcı dosyası bulundu. Tweet'ler sayılıyor...")
    
   
    for file_path in user_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:

                num_lines = sum(1 for line in f)
                tweet_count += num_lines
        except Exception as e:
            print(f"Uyarı: {file_path.name} dosyası okunurken bir hata oluştu: {e}")
            
    return user_count, tweet_count


print("Veri seti analizi başlıyor...")


pos_user_count, pos_tweet_count = count_stats(POS_IN)


neg_user_count, neg_tweet_count = count_stats(NEG_IN)


print("\n" + "="*30)
print("      VERİ SETİ ÖZETİ")
print("="*30)

print("\nPozitif Sınıf:")

print(f"  - Toplam Kullanıcı Sayısı: {pos_user_count:_d}")
print(f"  - Toplam Tweet Sayısı    : {pos_tweet_count:_d}")

print("\nNegatif Sınıf:")
print(f"  - Toplam Kullanıcı Sayısı: {neg_user_count:_d}")
print(f"  - Toplam Tweet Sayısı    : {neg_tweet_count:_d}")

print("\n" + "-"*30)

total_users = pos_user_count + neg_user_count
total_tweets = pos_tweet_count + neg_tweet_count

print("\nGenel Toplam:")
print(f"  - Toplam Kullanıcı Sayısı: {total_users:_d}")
print(f"  - Toplam Tweet Sayısı    : {total_tweets:_d}")
print("="*30)











import json
import re
from pathlib import Path
import shutil
import time

# --- DİZİNLER ---
# Üzerinde işlem yapılacak, filtrelenmiş verilerin olduğu klasör
BASE_DIR = Path(r"C:\Users\hc\Desktop\depression_detection\bert adımları")
POS_DIR = BASE_DIR / "pozitive"
NEG_DIR = BASE_DIR / "negative"

# --- TEMİZLEME FONKSİYONLARI ---

def clean_text_standart(text: str) -> str:
    """
    Genel BERT modelleri için 'Standart Temizlik' uygular (Plan A).
    - Metni küçük harfe çevirir.
    - URL, mention, hashtag, HTML etiketleri ve emoji/noktalama gibi unsurları tamamen kaldırır.
    """
    text = text.lower() # Metni küçük harfe çevir
    text = re.sub(r'<.*?>', '', text) # HTML etiketlerini sil
    text = re.sub(r'https?://\S+|www\.\S+', '', text) # URL'leri sil
    text = re.sub(r'@\w+', '', text) # Mention'ları sil
    text = re.sub(r'#\w+', '', text) # Hashtag'leri sil
    # Harf ve sayı olmayan her şeyi kaldır
    text = re.sub(r'[^a-z0-9\s]', '', text)
    # Birden fazla boşluğu tek boşluğa indir
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_text_advanced(text: str) -> str:
    """
    BERTweet/MentalBERT için 'Sade Temizlik' uygular (Plan B).
    - URL'leri 'HTTPURL' token'ı ile değiştirir.
    - Mention'ları '@user' token'ı ile değiştirir.
    - Hashtag, emoji, büyük/küçük harf gibi yapılara dokunmaz.
    """
    text = re.sub(r'<.*?>', '', text) # HTML etiketlerini sil
    # URL'leri 'HTTPURL' ile değiştir
    text = re.sub(r'https?://\S+|www\.\S+', 'HTTPURL', text)
    # Mention'ları '@user' ile değiştir
    text = re.sub(r'@\w+', '@user', text)
    # Birden fazla boşluğu tek boşluğa indir
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def update_files_with_cleaned_text(directory: Path):
    """
    Bir dizindeki tüm kullanıcı dosyalarını okur, her tweete iki yeni temizlenmiş
    metin alanı ekler ve dosyaları günceller.
    """
    if not directory.is_dir():
        print(f"HATA: '{directory}' dizini bulunamadı! İşlem atlanıyor.")
        return

    user_files = list(directory.glob('*.json'))
    total_files = len(user_files)
    print(f"'{directory.name}' dizinindeki {total_files:_d} kullanıcı dosyası güncelleniyor...")
    
    for i, user_file in enumerate(user_files):
        updated_lines = []
        try:
            with open(user_file, 'r', encoding='utf-8') as f_in:
                for line in f_in:
                    try:
                        tweet_data = json.loads(line)
                        original_text = tweet_data.get('text', '')

                        # Yeni alanları oluştur ve tweet verisine ekle
                        tweet_data['clean_text_standart'] = clean_text_standart(original_text)
                        tweet_data['clean_text_advanced'] = clean_text_advanced(original_text)
                        
                        # Güncellenmiş JSON satırını listeye ekle
                        updated_lines.append(json.dumps(tweet_data, ensure_ascii=False) + '\n')
                    except json.JSONDecodeError:
                        # Bozuk JSON satırları varsa, orijinal haliyle korumayı seçebiliriz
                        # veya tamamen atlayabiliriz. Şimdilik koruyalım.
                        updated_lines.append(line)
            
            # Dosyanın tamamını güncellenmiş içerikle yeniden yaz
            with open(user_file, 'w', encoding='utf-8') as f_out:
                f_out.writelines(updated_lines)

            # Kullanıcıya ilerleme hakkında bilgi ver
            print(f"  -> {i + 1} / {total_files} tamamlandı: {user_file.name}", end='\r')

        except Exception as e:
            print(f"\nHATA: {user_file.name} dosyası işlenirken bir hata oluştu: {e}")
    
    print(f"\n'{directory.name}' dizinindeki tüm dosyalar başarıyla güncellendi.")


# --- ANA İŞLEM ---
print("="*65)
print("ADIM 4: METİN TEMİZLEME (PLAN A ve PLAN B) İLE DOSYALARI GÜNCELLEME")
print("="*65)
print("Bu işlem mevcut dosyaların üzerine yazacaktır.")
start_time = time.time()

# Pozitif sınıf için işlemleri yap
update_files_with_cleaned_text(POS_DIR)

# Negatif sınıf için işlemleri yap
update_files_with_cleaned_text(NEG_DIR)

end_time = time.time()
print("\n" + "="*65)
print("İŞLEM TAMAMLANDI!")
print(f"Tüm .json dosyalarına 'clean_text_standart' ve 'clean_text_advanced' alanları eklendi.")
print(f"Toplam geçen süre: {end_time - start_time:.2f} saniye.")
print("="*65)




import os, json, shutil, random
from pathlib import Path
from collections import defaultdict, Counter

# =========================
# AYARLAR
# =========================
BASE_DIR   = Path(r"C:\Users\hc\Desktop\depression_detection\bert adımları")
SRC_POS    = BASE_DIR / "pozitive"
SRC_NEG    = BASE_DIR / "negative"

DEST_TRAIN = BASE_DIR / "train"
DEST_VAL   = BASE_DIR / "val"
DEST_TEST  = BASE_DIR / "test"

DROPPED    = BASE_DIR / "train_dropped"   # train undersampling'de çıkan fazlalar buraya taşınır
SEED       = 42

random.seed(SEED)

# =========================
# YARDIMCI FONKSİYONLAR
# =========================
def clean_and_make_dirs():
    for root in [DEST_TRAIN, DEST_VAL, DEST_TEST]:
        for cls in ["pozitive", "negative"]:
            d = root / cls
            if d.exists():
                shutil.rmtree(d)
            d.mkdir(parents=True, exist_ok=True)

def read_user_id_from_json(fp: Path) -> str | None:
    """
    JSONL veya satır satır JSON dosyalarında ilk geçerli satırdan common key'lerle user_id okur.
    """
    try:
        with open(fp, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue
                for k in ("user_id","userid","uid","author_id"):
                    v = obj.get(k, None)
                    if v is not None and str(v).strip():
                        return str(v).strip()
                # bazen user id nested olabilir; istersen buraya ek kural yazılabilir
                break
    except Exception:
        pass
    return None

def fallback_user_id_from_filename(fp: Path) -> str:
    # Son çare: filename gövdesi (ama asıl amaç JSON içini kullanmak)
    stem = fp.stem
    return stem.split("created_at")[0] if "created_at" in stem else stem

def get_user_id(fp: Path) -> str:
    uid = read_user_id_from_json(fp)
    return uid if uid else fallback_user_id_from_filename(fp)

def scan_class_folder(src_dir: Path) -> dict[str, list[Path]]:
    """
    Sınıf klasörünü tarar: user_id -> [dosyalar]
    """
    bucket = defaultdict(list)
    for p in src_dir.iterdir():
        if p.is_file() and p.suffix.lower()==".json":
            uid = get_user_id(p)
            bucket[uid].append(p)
    return bucket

def copy_files(files: list[Path], dst: Path):
    dst.mkdir(parents=True, exist_ok=True)
    for fp in files:
        shutil.copy2(fp, dst / fp.name)

def move_files(files: list[Path], dst: Path):
    dst.mkdir(parents=True, exist_ok=True)
    for fp in files:
        shutil.move(str(fp), str(dst / fp.name))

def count_files(d: Path) -> int:
    return len(list(d.glob("*.json")))

def user_sets_in_split(root: Path) -> tuple[set[str], set[str], set[str]]:
    def users_in(dir_):
        s = set()
        for fp in dir_.glob("*.json"):
            s.add(get_user_id(fp))
        return s
    return users_in(root / "pozitive"), users_in(root / "negative"), users_in(root)  # sonuncusu tümü

# =========================
# USER-LEVEL 80/10/10 (DOSYA HEDEFİNE GREEDY)
# =========================
def user_level_split_by_file_targets(user2files: dict[str, list[Path]], r_train=0.8, r_val=0.1, r_test=0.1):
    """
    Her kullanıcı tek split'e atanır. Hedef: toplam dosya sayısı oranlarını yakalamak.
    Greedy: her adımda dosya sayısı hedeften en eksik olan split'e kullanıcıyı koy.
    """
    # hedef dosya sayısı
    total_files = sum(len(v) for v in user2files.values())
    t_train = round(total_files * r_train)
    t_val   = round(total_files * r_val)
    t_test  = total_files - t_train - t_val

    # kullanıcıları rastgele sırala (deterministik seed için listeyi dışarıda kar)
    items = list(user2files.items())
    random.Random(SEED).shuffle(items)

    # sayaçlar
    cur = {"train":0, "val":0, "test":0}
    T   = {"train":t_train, "val":t_val, "test":t_test}
    assign = {"train":[], "val":[], "test":[]}

    def need_ratio(split):  # ne kadar eksik: cur/target oranı (düşük olan daha eksik)
        # target 0 olamaz ama guard koyuyoruz
        tgt = max(T[split], 1)
        return cur[split]/tgt

    for uid, files in items:
        # en çok eksik olan split’e ata
        split = min(("train","val","test"), key=need_ratio)
        assign[split].append(uid)
        cur[split] += len(files)

    return assign, T, cur

# =========================
# ANA AKIŞ
# =========================
print("▶ Hedef klasörler temizleniyor/oluşturuluyor...")
clean_and_make_dirs()

# 1) Kaynakları tara (user -> dosyalar)
pos_u2f = scan_class_folder(SRC_POS)
neg_u2f = scan_class_folder(SRC_NEG)

print(f"[INFO] poz users: {len(pos_u2f)} | neg users: {len(neg_u2f)}")
print(f"[INFO] poz files: {sum(len(v) for v in pos_u2f.values())} | neg files: {sum(len(v) for v in neg_u2f.values())}")

# 2) Sınıf bazında user-level split (dosya hedefli)
pos_assign, pos_target, pos_cur = user_level_split_by_file_targets(pos_u2f, 0.8, 0.1, 0.1)
neg_assign, neg_target, neg_cur = user_level_split_by_file_targets(neg_u2f, 0.8, 0.1, 0.1)

print("[TARGET poz] train/val/test:", pos_target, " | [actual after assign]:", pos_cur)
print("[TARGET neg] train/val/test:", neg_target, " | [actual after assign]:", neg_cur)

# 3) Kopyala
copy_files([f for u in pos_assign["train"] for f in pos_u2f[u]], DEST_TRAIN / "pozitive")
copy_files([f for u in pos_assign["val"]   for f in pos_u2f[u]], DEST_VAL   / "pozitive")
copy_files([f for u in pos_assign["test"]  for f in pos_u2f[u]], DEST_TEST  / "pozitive")

copy_files([f for u in neg_assign["train"] for f in neg_u2f[u]], DEST_TRAIN / "negative")
copy_files([f for u in neg_assign["val"]   for f in neg_u2f[u]], DEST_VAL   / "negative")
copy_files([f for u in neg_assign["test"]  for f in neg_u2f[u]], DEST_TEST  / "negative")

print("\n--- SPLIT SONRASI (dosya sayıları) ---")
print("TRAIN/pozitive:", count_files(DEST_TRAIN / "pozitive"))
print("TRAIN/negative:", count_files(DEST_TRAIN / "negative"))
print("VAL/pozitive  :", count_files(DEST_VAL   / "pozitive"))
print("VAL/negative  :", count_files(DEST_VAL   / "negative"))
print("TEST/pozitive :", count_files(DEST_TEST  / "pozitive"))
print("TEST/negative :", count_files(DEST_TEST  / "negative"))

# 4) Overlap kontrolü (user-level) — her split kendi içinde pos/neg birleşimi
tr_pos_u, tr_neg_u, tr_all = user_sets_in_split(DEST_TRAIN)
va_pos_u, va_neg_u, va_all = user_sets_in_split(DEST_VAL)
te_pos_u, te_neg_u, te_all = user_sets_in_split(DEST_TEST)

def report_overlap(a: set[str], b: set[str], name: str):
    inter = a & b
    print(f"[OVERLAP] {name}: {len(inter)}")
    if inter:
        print("  örnekler:", list(sorted(inter))[:10])

print("\n[USER OVERLAP KONTROL]")
report_overlap(tr_all, va_all, "train∩val")
report_overlap(tr_all, te_all, "train∩test")
report_overlap(va_all, te_all, "val∩test")

# 5) SADECE TRAIN için dosya-bazlı UNDERSAMPLING (poz=neg eşit dosya sayısı)
print("\n▶ TRAIN undersampling (dosya sayısı eşitleme)...")
train_pos_files = list((DEST_TRAIN / "pozitive").glob("*.json"))
train_neg_files = list((DEST_TRAIN / "negative").glob("*.json"))
n_pos, n_neg = len(train_pos_files), len(train_neg_files)
target = min(n_pos, n_neg)

def undersample_dir_keep_target(dir_path: Path, target_n: int, dropped_subdir: Path):
    files = list(dir_path.glob("*.json"))
    if len(files) <= target_n:
        return 0
    random.Random(SEED).shuffle(files)
    keep = set(files[:target_n])
    drop = [f for f in files if f not in keep]
    move_files(drop, dropped_subdir)
    return len(drop)

dropped_pos = dropped_neg = 0
if n_pos > target:
    dropped_pos = undersample_dir_keep_target(DEST_TRAIN / "pozitive", target, DROPPED / "pozitive")
if n_neg > target:
    dropped_neg = undersample_dir_keep_target(DEST_TRAIN / "negative", target, DROPPED / "negative")

print(f"[DONE] dropped -> poz:{dropped_pos} | neg:{dropped_neg}")
print("\n=== FİNAL (TRAIN balanced, VAL/TEST sabit) ===")
print("TRAIN/pozitive:", count_files(DEST_TRAIN / "pozitive"))
print("TRAIN/negative:", count_files(DEST_TRAIN / "negative"))
print("VAL/pozitive  :", count_files(DEST_VAL   / "pozitive"))
print("VAL/negative  :", count_files(DEST_VAL   / "negative"))
print("TEST/pozitive :", count_files(DEST_TEST  / "pozitive"))
print("TEST/negative :", count_files(DEST_TEST  / "negative"))

