import json
from pathlib import Path


POS_IN = Path()
NEG_IN = Path()

def count_stats(directory_path: Path):
    
    if not directory_path.is_dir():
        print(f"HATA: '{directory_path}' dizini bulunamadı!")
        return 0, 0
        
  
    user_files = list(directory_path.glob('*.json'))
    user_count = len(user_files)
    tweet_count = 0
    
    print(f"'{directory_path.name}' dizininde {user_count} kullanıcı dosyası bulundu. Tweet'ler sayılıyor...")
    
   
    for file_path in user_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:

                num_lines = sum(1 for line in f)
                tweet_count += num_lines
        except Exception as e:
            print(f"Uyarı: {file_path.name} dosyası okunurken bir hata oluştu: {e}")
            
    return user_count, tweet_count


print("Veri seti analizi başlıyor...")


pos_user_count, pos_tweet_count = count_stats(POS_IN)


neg_user_count, neg_tweet_count = count_stats(NEG_IN)


print("\n" + "="*30)
print("      VERİ SETİ ÖZETİ")
print("="*30)

print("\nPozitif Sınıf:")

print(f"  - Toplam Kullanıcı Sayısı: {pos_user_count:_d}")
print(f"  - Toplam Tweet Sayısı    : {pos_tweet_count:_d}")

print("\nNegatif Sınıf:")
print(f"  - Toplam Kullanıcı Sayısı: {neg_user_count:_d}")
print(f"  - Toplam Tweet Sayısı    : {neg_tweet_count:_d}")

print("\n" + "-"*30)

total_users = pos_user_count + neg_user_count
total_tweets = pos_tweet_count + neg_tweet_count

print("\nGenel Toplam:")
print(f"  - Toplam Kullanıcı Sayısı: {total_users:_d}")
print(f"  - Toplam Tweet Sayısı    : {total_tweets:_d}")
print("="*30)











import json
import re
from pathlib import Path
import shutil
import time

# --- DİZİNLER ---
# Üzerinde işlem yapılacak, filtrelenmiş verilerin olduğu klasör
BASE_DIR = Path(r"C:\Users\hc\Desktop\depression_detection\bert adımları")
POS_DIR = BASE_DIR / "pozitive"
NEG_DIR = BASE_DIR / "negative"

# --- TEMİZLEME FONKSİYONLARI ---

def clean_text_standart(text: str) -> str:
    """
    Genel BERT modelleri için 'Standart Temizlik' uygular (Plan A).
    - Metni küçük harfe çevirir.
    - URL, mention, hashtag, HTML etiketleri ve emoji/noktalama gibi unsurları tamamen kaldırır.
    """
    text = text.lower() # Metni küçük harfe çevir
    text = re.sub(r'<.*?>', '', text) # HTML etiketlerini sil
    text = re.sub(r'https?://\S+|www\.\S+', '', text) # URL'leri sil
    text = re.sub(r'@\w+', '', text) # Mention'ları sil
    text = re.sub(r'#\w+', '', text) # Hashtag'leri sil
    # Harf ve sayı olmayan her şeyi kaldır
    text = re.sub(r'[^a-z0-9\s]', '', text)
    # Birden fazla boşluğu tek boşluğa indir
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_text_advanced(text: str) -> str:
    """
    BERTweet/MentalBERT için 'Sade Temizlik' uygular (Plan B).
    - URL'leri 'HTTPURL' token'ı ile değiştirir.
    - Mention'ları '@user' token'ı ile değiştirir.
    - Hashtag, emoji, büyük/küçük harf gibi yapılara dokunmaz.
    """
    text = re.sub(r'<.*?>', '', text) # HTML etiketlerini sil
    # URL'leri 'HTTPURL' ile değiştir
    text = re.sub(r'https?://\S+|www\.\S+', 'HTTPURL', text)
    # Mention'ları '@user' ile değiştir
    text = re.sub(r'@\w+', '@user', text)
    # Birden fazla boşluğu tek boşluğa indir
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def update_files_with_cleaned_text(directory: Path):
    """
    Bir dizindeki tüm kullanıcı dosyalarını okur, her tweete iki yeni temizlenmiş
    metin alanı ekler ve dosyaları günceller.
    """
    if not directory.is_dir():
        print(f"HATA: '{directory}' dizini bulunamadı! İşlem atlanıyor.")
        return

    user_files = list(directory.glob('*.json'))
    total_files = len(user_files)
    print(f"'{directory.name}' dizinindeki {total_files:_d} kullanıcı dosyası güncelleniyor...")
    
    for i, user_file in enumerate(user_files):
        updated_lines = []
        try:
            with open(user_file, 'r', encoding='utf-8') as f_in:
                for line in f_in:
                    try:
                        tweet_data = json.loads(line)
                        original_text = tweet_data.get('text', '')

                        # Yeni alanları oluştur ve tweet verisine ekle
                        tweet_data['clean_text_standart'] = clean_text_standart(original_text)
                        tweet_data['clean_text_advanced'] = clean_text_advanced(original_text)
                        
                        # Güncellenmiş JSON satırını listeye ekle
                        updated_lines.append(json.dumps(tweet_data, ensure_ascii=False) + '\n')
                    except json.JSONDecodeError:
                        # Bozuk JSON satırları varsa, orijinal haliyle korumayı seçebiliriz
                        # veya tamamen atlayabiliriz. Şimdilik koruyalım.
                        updated_lines.append(line)
            
            # Dosyanın tamamını güncellenmiş içerikle yeniden yaz
            with open(user_file, 'w', encoding='utf-8') as f_out:
                f_out.writelines(updated_lines)

            # Kullanıcıya ilerleme hakkında bilgi ver
            print(f"  -> {i + 1} / {total_files} tamamlandı: {user_file.name}", end='\r')

        except Exception as e:
            print(f"\nHATA: {user_file.name} dosyası işlenirken bir hata oluştu: {e}")
    
    print(f"\n'{directory.name}' dizinindeki tüm dosyalar başarıyla güncellendi.")


# --- ANA İŞLEM ---
print("="*65)
print("ADIM 4: METİN TEMİZLEME (PLAN A ve PLAN B) İLE DOSYALARI GÜNCELLEME")
print("="*65)
print("Bu işlem mevcut dosyaların üzerine yazacaktır.")
start_time = time.time()

# Pozitif sınıf için işlemleri yap
update_files_with_cleaned_text(POS_DIR)

# Negatif sınıf için işlemleri yap
update_files_with_cleaned_text(NEG_DIR)

end_time = time.time()
print("\n" + "="*65)
print("İŞLEM TAMAMLANDI!")
print(f"Tüm .json dosyalarına 'clean_text_standart' ve 'clean_text_advanced' alanları eklendi.")
print(f"Toplam geçen süre: {end_time - start_time:.2f} saniye.")
print("="*65)
